{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset. It contains translations from English to Spanish, so swap the order of the phrases. Also add `\\t` and `\\n` as the start and stop tokens in the target sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"\\t\"\n",
    "stop_token = \"\\n\"\n",
    "\n",
    "with open(\"data/spa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    samples = f.read().split(\"\\n\")\n",
    "\n",
    "samples = [sample.strip().split(\"\\t\")\n",
    "           for sample in samples if len(sample.strip()) > 0]\n",
    "\n",
    "samples = [(es, start_token + en + stop_token)\n",
    "           for en, es in samples if len(es) < 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_samples, valid_samples = train_test_split(samples, train_size=.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the training vocabulary. Those are the only tokens you can trust the model will know how to handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vocab = set()\n",
    "out_vocab = set()\n",
    "\n",
    "for in_seq, out_seq in train_samples:\n",
    "    in_vocab.update(in_seq)\n",
    "    out_vocab.update(out_seq)\n",
    "    \n",
    "in_vocab_size = len(in_vocab)\n",
    "out_vocab_size = len(out_vocab)\n",
    "\n",
    "print(\"Input vocab size:\", in_vocab_size)\n",
    "print(\"Output vocab size:\", out_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(in_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(out_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through validation set and remove any tokens not present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_samples = []\n",
    "for in_seq, out_seq in valid_samples:\n",
    "    tmp_in_seq = [c for c in in_seq if c in in_vocab]\n",
    "    tmp_out_seq = [c for c in out_seq if c in out_vocab]\n",
    "\n",
    "    tmp_samples.append((\"\".join(tmp_in_seq), \"\".join(tmp_out_seq)))\n",
    "    \n",
    "valid_samples = tmp_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build sequence-to-sequence model with bidirectional encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Concatenate, Dense, Input, GRU, Masking\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encoder with two GRU layers, each processing the input in a different direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 512\n",
    "\n",
    "encoder_in = Input(shape=(None, in_vocab_size), name=\"encoder_in\")\n",
    "encoder_mask = Masking(name=\"encoder_mask\")(encoder_in)\n",
    "\n",
    "fwd_enc_gru = GRU(latent_dim, recurrent_dropout=0.3, name=\"fwd_enc_gru\")\n",
    "rev_enc_gru = GRU(latent_dim, go_backwards=True, recurrent_dropout=0.3, name=\"rev_enc_gru\")\n",
    "fwd_enc_out = fwd_enc_gru(encoder_mask)\n",
    "rev_enc_out = rev_enc_gru(encoder_mask)\n",
    "\n",
    "encoder_out = Concatenate(name=\"encoder_out\")([fwd_enc_out, rev_enc_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder's input dimensions are twice as big as the encoder's output dimensions because you combine the forward and reverse outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_latent_dim = latent_dim * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_in = Input(shape=(None, out_vocab_size), name=\"decoder_in\")\n",
    "\n",
    "decoder_mask = Masking(name=\"decoder_mask\")(decoder_in)\n",
    "decoder_gru = GRU(decoder_latent_dim, return_sequences=True,\n",
    "                  return_state=True, dropout=0.2, recurrent_dropout=0.3,\n",
    "                  name=\"decoder_gru\")\n",
    "decoder_gru_out, _ = decoder_gru(decoder_mask, initial_state=encoder_out)\n",
    "decoder_dense = Dense(out_vocab_size, activation=\"softmax\", name=\"decoder_out\")\n",
    "decoder_out = decoder_dense(decoder_gru_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = Model([encoder_in, decoder_in], decoder_out)\n",
    "seq2seq_model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create maps to convert characters to and from ints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_token2int = {token : i for i, token in enumerate(sorted(in_vocab))}\n",
    "out_token2int = {token : i for i, token in enumerate(sorted(out_vocab))}\n",
    "out_int2token = {i : token for (token, i) in out_token2int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions for one-hot encoding sequences for use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_batch_storage(batch_size, in_seq_len, out_seq_len):\n",
    "    \n",
    "    enc_in_seqs = np.zeros(\n",
    "        (batch_size, in_seq_len, in_vocab_size),\n",
    "        dtype=np.float32)\n",
    "\n",
    "    dec_in_seqs = np.zeros(\n",
    "        (batch_size, out_seq_len, out_vocab_size),\n",
    "        dtype=np.float32)\n",
    "\n",
    "    dec_out_seqs = np.zeros(\n",
    "        (batch_size, out_seq_len, out_vocab_size),\n",
    "        dtype=np.float32)\n",
    "        \n",
    "    return enc_in_seqs, dec_in_seqs, dec_out_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(samples):\n",
    "    batch_size = len(samples)\n",
    "    max_in_length = max([len(seq) for seq, _ in samples])\n",
    "    max_out_length = max([len(seq) for _, seq in samples])\n",
    "\n",
    "    enc_in_seqs, dec_in_seqs, dec_out_seqs = make_batch_storage(\n",
    "        batch_size, max_in_length, max_out_length)\n",
    "    \n",
    "    for i, (in_seq, out_seq) in enumerate(samples):\n",
    "        for time_step, token in enumerate(in_seq):\n",
    "            enc_in_seqs[i, time_step, in_token2int[token]] = 1\n",
    "\n",
    "        for time_step, token in enumerate(out_seq):\n",
    "            dec_in_seqs[i, time_step, out_token2int[token]] = 1\n",
    "\n",
    "        for time_step, token in enumerate(out_seq[1:]):\n",
    "            dec_out_seqs[i, time_step, out_token2int[token]] = 1\n",
    "            \n",
    "    return enc_in_seqs, dec_in_seqs, dec_out_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_util import Seq2SeqBatchGenerator\n",
    "\n",
    "batch_size = 64\n",
    "train_generator = Seq2SeqBatchGenerator(train_samples, batch_size, encode_batch)\n",
    "valid_generator = Seq2SeqBatchGenerator(valid_samples, batch_size, encode_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning:\n",
    "\n",
    "Running the following cell can take a long time. If you are just experimenting, consider changing the `epochs` value to something small, from `1` to `10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "seq2seq_model.fit_generator(train_generator, epochs=500,\n",
    "                            validation_data=valid_generator,\n",
    "                            callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encoder/decoder models for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_encoder = Model(encoder_in, encoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_dec_h_in = Input(shape=(decoder_latent_dim,), name=\"decoder_h_in\")\n",
    "\n",
    "inf_dec_gru_out, inf_dec_h_out = decoder_gru(\n",
    "    decoder_in, initial_state=inf_dec_h_in)\n",
    "\n",
    "inf_dec_out = decoder_dense(inf_dec_gru_out)\n",
    "\n",
    "inf_decoder = Model(\n",
    "    [decoder_in, inf_dec_h_in],\n",
    "    [inf_dec_out, inf_dec_h_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test trained model on the first 100 samples from both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_out_seq_len = max([len(seq) for _, seq in samples])\n",
    "print(\"Max output length: \", max_out_seq_len)\n",
    "\n",
    "start_token_idx = out_token2int[start_token]\n",
    "stop_token_idx = out_token2int[stop_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sequence(one_hot_seq, encoder, decoder):\n",
    "    encoding = encoder.predict(one_hot_seq)\n",
    "    \n",
    "    decoder_in = np.zeros((1, 1, out_vocab_size), dtype=np.float32)\n",
    "\n",
    "    translated_text = ''\n",
    "    done_decoding = False\n",
    "    decoded_idx = start_token_idx\n",
    "    while not done_decoding:\n",
    "        decoder_in[0, 0, decoded_idx] = 1\n",
    "        decoding, encoding = decoder.predict([decoder_in, encoding])\n",
    "        decoder_in[0, 0, decoded_idx] = 0\n",
    "\n",
    "        decoded_idx = np.argmax(decoding[0, -1, :])\n",
    "        \n",
    "        if decoded_idx == stop_token_idx:\n",
    "            done_decoding = True\n",
    "        else:\n",
    "            translated_text += out_int2token[decoded_idx]\n",
    "\n",
    "        if len(translated_text) >= max_out_seq_len:\n",
    "            done_decoding = True\n",
    "            \n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_util import test_predictions\n",
    "\n",
    "test_predictions(valid_samples[:100], inf_encoder, inf_decoder, encode_batch, translate_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions(train_samples[:100], inf_encoder, inf_decoder, encode_batch, translate_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export model in Core ML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_enc_in = Input(shape=(None, in_vocab_size), name=\"encoder_in\")\n",
    "\n",
    "coreml_fwd_enc_gru = GRU(latent_dim, name=\"fwd_enc_gru\")\n",
    "coreml_rev_enc_gru = GRU(latent_dim, go_backwards=True, name=\"rev_enc_gru\")\n",
    "\n",
    "coreml_fwd_out = coreml_fwd_enc_gru(coreml_enc_in)\n",
    "coreml_rev_out = coreml_rev_enc_gru(coreml_enc_in)\n",
    "\n",
    "coreml_enc_out = Concatenate(name=\"encoder_out\")([coreml_fwd_out, coreml_rev_out])\n",
    "\n",
    "coreml_encoder_model = Model(coreml_enc_in, coreml_enc_out)\n",
    "coreml_encoder_model.output_layers = coreml_encoder_model._output_layers\n",
    "\n",
    "inf_encoder.save_weights(\"Es2EnBidirGruCharEncoderWeights.h5\")\n",
    "coreml_encoder_model.load_weights(\"Es2EnBidirGruCharEncoderWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools\n",
    "\n",
    "coreml_encoder = coremltools.converters.keras.convert(\n",
    "    coreml_encoder_model,\n",
    "    input_names=\"oneHotEncodedSeq\",\n",
    "    output_names=\"decodersIntialState\")\n",
    "\n",
    "coreml_encoder.save(\"Es2EnBidirGruCharEncoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_dec_in = Input(shape=(None, out_vocab_size))\n",
    "\n",
    "coreml_dec_gru = GRU(decoder_latent_dim, return_sequences=True, return_state=True, name=\"decoder_gru\")\n",
    "coreml_dec_gru_out, _ = coreml_dec_gru(coreml_dec_in)\n",
    "coreml_dec_dense = Dense(out_vocab_size, activation=\"softmax\")\n",
    "coreml_dec_out = coreml_dec_dense(coreml_dec_gru_out)\n",
    "\n",
    "coreml_decoder_model = Model(coreml_dec_in, coreml_dec_out)\n",
    "coreml_decoder_model.output_layers = coreml_decoder_model._output_layers\n",
    "\n",
    "inf_decoder.save_weights(\"Es2EnBidirGruCharDecoderWeights.h5\")\n",
    "coreml_decoder_model.load_weights(\"Es2EnBidirGruCharDecoderWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coreml_decoder = coremltools.converters.keras.convert(\n",
    "    coreml_decoder_model,\n",
    "    input_names=\"encodedChar\",\n",
    "    output_names=\"nextCharProbs\")\n",
    "\n",
    "coreml_decoder.save(\"Es2EnBidirGruCharDecoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert weights to 16bit floats. This shouldn't hurt performance much, if at all, and it reduces the app's download size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_fp16(mlmodel_filename):\n",
    "    basename = mlmodel_filename[:-len(\".mlmodel\")]\n",
    "    spec = coremltools.utils.load_spec(mlmodel_filename)\n",
    "    spec_16bit = \\\n",
    "      coremltools.utils.convert_neural_network_spec_weights_to_fp16(spec)\n",
    "    coremltools.utils.save_spec(spec_16bit, f\"{basename}16Bit.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_fp16(\"Es2EnBidirGruCharEncoder.mlmodel\")\n",
    "convert_to_fp16(\"Es2EnBidirGruCharDecoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"esCharToInt.json\", \"w\") as f:\n",
    "    json.dump(in_token2int, f)\n",
    "with open(\"intToEnChar.json\", \"w\") as f:\n",
    "    json.dump(out_int2token, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
